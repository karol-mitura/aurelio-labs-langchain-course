{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AsMFKCALr3y"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/07-lcel.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PQji83qLpVC"
   },
   "source": [
    "#### LangChain Essentials Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22dLBWVZLpVD"
   },
   "source": [
    "# LangChains Expression Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiRUqmfBLpVE"
   },
   "source": [
    "LangChain is one of the most popular open source libraries for AI Engineers. It's goal is to abstract away the complexity in building AI software, provide easy-to-use building blocks, and make it easier when switching between AI service providers.\n",
    "\n",
    "In this example, we will introduce LangChain's Expression Langauge (LCEL), abstracting a full chain and understanding how it will work. We'll provide examples for both OpenAI's `gpt-4o-mini` *and* Meta's `llama3.2` via Ollama!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "i690tIabLwb_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\karol\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU \\\n",
    "  langchain-core==0.3.33 \\\n",
    "  langchain-openai==0.3.3 \\\n",
    "  langchain-community==0.3.16 \\\n",
    "  langsmith==0.3.4 \\\n",
    "  docarray==0.40.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOR87lEpLpVE"
   },
   "source": [
    "---\n",
    "\n",
    "> ⚠️ We will be using OpenAI for this example allowing us to run everything via API. If you would like to use Ollama instead, check out the [Ollama LangChain Course](https://github.com/aurelio-labs/langchain-course/tree/main/notebooks/ollama).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfpD-R6sLpVE"
   },
   "source": [
    "---\n",
    "\n",
    "> ⚠️ If using LangSmith, add your API key below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "exBqQHgqLpVE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# must enter API key\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"aurelioai-langchain-course-langsmith-openai\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYGeByKjLpVF"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQbgErgdLpVF"
   },
   "source": [
    "## Traditional Chains vs LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dyluns_ELpVF"
   },
   "source": [
    "In this section we're going to dive into a basic example using the traditional method for building chains before jumping into LCEL. We will build a pipeline where the user must input a specific topic, and then the LLM will look and return a report on the specified topic. Generating a _research report_ for the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTZTTiK7LpVF"
   },
   "source": [
    "### Traditional LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDHfbKZ7LpVF"
   },
   "source": [
    "The `LLMChain` is the simplest chain originally introduced in LangChain. This chain takes a prompt, feeds it into an LLM, and _optionally_ adds an output parsing step before returning the result.\n",
    "\n",
    "Let's see how we construct this using the traditional method, for this we need:\n",
    "\n",
    "* `prompt` — a `PromptTemplate` that will be used to generate the prompt for the LLM.\n",
    "* `llm` — the LLM we will be using to generate the output.\n",
    "* `output_parser` — an optional output parser that will be used to parse the structured output of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Jyz2vWLoLpVF"
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"Give me a small report on {topic}\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rykro39YLpVF"
   },
   "source": [
    "For the LLM, we'll start by initializing our connection to the OpenAI API. We do need an OpenAI API key, which you can get from the [OpenAI platform](https://platform.openai.com/api-keys).\n",
    "\n",
    "We will use the `gpt-4o-mini` model with a `temperature` of `0.0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Uvs7hILRLpVF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \\\n",
    "    or getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhs6_3c4LpVF",
    "outputId": "b7051222-b74e-45c0-d19f-ac28a6bb87fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-d2f8a41d-daa0-4ea8-a60b-27622ead4066-0', usage_metadata={'input_tokens': 9, 'output_tokens': 9, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_out = llm.invoke(\"Hello there\")\n",
    "llm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V39qjMpLLpVG"
   },
   "source": [
    "Then we define our output parser, this will be used to parse the output of the LLM. In this case, we will use the `StrOutputParser` which will parse the `AIMessage` output from our LLM into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "46qTjklnLpVG"
   },
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "z0uwxedBLpVG",
    "outputId": "faa83a90-aa81-40ba-b9a9-625507c7b7be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = output_parser.invoke(llm_out)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mk4CUy52LpVG"
   },
   "source": [
    "Through the `LLMChain` class we can place each of our components into a linear `chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PrFk1J9dLpVG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_10636\\2960353250.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjNsVMVpLpVG"
   },
   "source": [
    "Note that the `LLMChain` _was_ deprecated in LangChain `0.1.17`, the expected way of constructing these chains today is through LCEL, which we'll cover in a moment.\n",
    "\n",
    "We can `invoke` our `chain`, providing a `topic` that we'd like to be researched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lDDq8Cq7LpVG",
    "outputId": "e30de2aa-7005-45f4-cb96-e681e9c2c1d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'retrieval augmented generation',\n",
       " 'text': '### Report on Retrieval-Augmented Generation (RAG)\\n\\n#### Introduction\\nRetrieval-Augmented Generation (RAG) is an advanced approach in natural language processing (NLP) that combines the strengths of information retrieval and generative models. This technique enhances the capabilities of language models by allowing them to access external knowledge sources, thereby improving the accuracy and relevance of generated responses.\\n\\n#### Concept Overview\\nRAG operates on the principle of integrating a retrieval mechanism with a generative model. The process typically involves two main components:\\n\\n1. **Retrieval Component**: This part of the system retrieves relevant documents or pieces of information from a large corpus based on a given query. It often employs techniques such as vector embeddings and similarity search to identify the most pertinent data.\\n\\n2. **Generative Component**: Once relevant information is retrieved, a generative model (often based on architectures like Transformers) processes this information to produce coherent and contextually appropriate text. The generative model can be fine-tuned to ensure that the output is not only relevant but also stylistically consistent with the desired output.\\n\\n#### Advantages of RAG\\n- **Enhanced Knowledge Access**: By leveraging external databases or knowledge bases, RAG can provide more accurate and up-to-date information than models relying solely on pre-trained knowledge.\\n- **Improved Contextual Relevance**: The retrieval step allows the model to ground its responses in specific, relevant content, leading to more contextually appropriate outputs.\\n- **Scalability**: RAG systems can scale to incorporate vast amounts of information, making them suitable for applications requiring extensive knowledge bases.\\n\\n#### Applications\\nRAG has a wide range of applications, including:\\n- **Question Answering**: Providing precise answers to user queries by retrieving relevant documents and generating responses based on that information.\\n- **Chatbots and Virtual Assistants**: Enhancing conversational agents with the ability to pull in real-time information from external sources.\\n- **Content Creation**: Assisting in generating articles, reports, or summaries by retrieving relevant data and synthesizing it into coherent text.\\n\\n#### Challenges\\nDespite its advantages, RAG also faces several challenges:\\n- **Quality of Retrieved Information**: The effectiveness of the generative model heavily depends on the quality and relevance of the retrieved documents.\\n- **Complexity of Integration**: Combining retrieval and generation components can introduce complexity in system design and implementation.\\n- **Latency**: The retrieval process can add latency to response times, which may be a concern in real-time applications.\\n\\n#### Conclusion\\nRetrieval-Augmented Generation represents a significant advancement in the field of NLP, merging the capabilities of information retrieval with generative modeling. By enabling models to access and utilize external knowledge, RAG enhances the relevance and accuracy of generated content, making it a powerful tool for various applications. As research and development in this area continue, we can expect further improvements in the efficiency and effectiveness of RAG systems.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(\"retrieval augmented generation\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbHqc1qLLpVG"
   },
   "source": [
    "We can view a formatted version of this output using the `Markdown` display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 980
    },
    "id": "hBxPHOZ-LpVG",
    "outputId": "23c54f43-dc98-4c56-a47f-4a996b206eb7"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Report on Retrieval-Augmented Generation (RAG)\n",
       "\n",
       "#### Introduction\n",
       "Retrieval-Augmented Generation (RAG) is an advanced approach in natural language processing (NLP) that combines the strengths of information retrieval and generative models. This technique enhances the capabilities of language models by allowing them to access external knowledge sources, thereby improving the accuracy and relevance of generated responses.\n",
       "\n",
       "#### Concept Overview\n",
       "RAG operates on the principle of integrating a retrieval mechanism with a generative model. The process typically involves two main components:\n",
       "\n",
       "1. **Retrieval Component**: This part of the system retrieves relevant documents or pieces of information from a large corpus based on a given query. It often employs techniques such as vector embeddings and similarity search to identify the most pertinent data.\n",
       "\n",
       "2. **Generative Component**: Once relevant information is retrieved, a generative model (often based on architectures like Transformers) processes this information to produce coherent and contextually appropriate text. The generative model can be fine-tuned to ensure that the output is not only relevant but also stylistically consistent with the desired output.\n",
       "\n",
       "#### Advantages of RAG\n",
       "- **Enhanced Knowledge Access**: By leveraging external databases or knowledge bases, RAG can provide more accurate and up-to-date information than models relying solely on pre-trained knowledge.\n",
       "- **Improved Contextual Relevance**: The retrieval step allows the model to ground its responses in specific, relevant content, leading to more contextually appropriate outputs.\n",
       "- **Scalability**: RAG systems can scale to incorporate vast amounts of information, making them suitable for applications requiring extensive knowledge bases.\n",
       "\n",
       "#### Applications\n",
       "RAG has a wide range of applications, including:\n",
       "- **Question Answering**: Providing precise answers to user queries by retrieving relevant documents and generating responses based on that information.\n",
       "- **Chatbots and Virtual Assistants**: Enhancing conversational agents with the ability to pull in real-time information from external sources.\n",
       "- **Content Creation**: Assisting in generating articles, reports, or summaries by retrieving relevant data and synthesizing it into coherent text.\n",
       "\n",
       "#### Challenges\n",
       "Despite its advantages, RAG also faces several challenges:\n",
       "- **Quality of Retrieved Information**: The effectiveness of the generative model heavily depends on the quality and relevance of the retrieved documents.\n",
       "- **Complexity of Integration**: Combining retrieval and generation components can introduce complexity in system design and implementation.\n",
       "- **Latency**: The retrieval process can add latency to response times, which may be a concern in real-time applications.\n",
       "\n",
       "#### Conclusion\n",
       "Retrieval-Augmented Generation represents a significant advancement in the field of NLP, merging the capabilities of information retrieval with generative modeling. By enabling models to access and utilize external knowledge, RAG enhances the relevance and accuracy of generated content, making it a powerful tool for various applications. As research and development in this area continue, we can expect further improvements in the efficiency and effectiveness of RAG systems."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(result[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbmq0DfeLpVG"
   },
   "source": [
    "That is a simple `LLMChain` using the traditional LangChain method. Now let's move onto LCEL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWets8OpLpVG"
   },
   "source": [
    "## LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Lr5NA0xLpVG"
   },
   "source": [
    "**L**ang**C**hain **E**xpression **L**anguage (LCEL) is the recommended approach to building chains in LangChain. Having superceeded the traditional methods with `LLMChain`, etc. LCEL gives us a more flexible system for building chains. The pipe operator `|` is used by LCEL to _chain_ together components. Let's see how we'd construct an `LLMChain` using LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8R-9b7ulLpVH"
   },
   "outputs": [],
   "source": [
    "lcel_chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdJvzetzLpVH"
   },
   "source": [
    "We can `invoke` this chain in the same way as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "XKWzIwpCLpVH",
    "outputId": "9dc83fdd-98a4-4513-dcac-055092aeeb12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Report on Retrieval-Augmented Generation (RAG)\\n\\n#### Introduction\\nRetrieval-Augmented Generation (RAG) is an innovative approach that combines the strengths of information retrieval and natural language generation. This method enhances the capabilities of language models by allowing them to access external knowledge sources, thereby improving the accuracy and relevance of generated responses.\\n\\n#### Concept Overview\\nRAG operates on the principle of integrating a retrieval mechanism with a generative model. The process typically involves two main components:\\n\\n1. **Retrieval Component**: This part of the system retrieves relevant documents or pieces of information from a large corpus based on a given query. It uses techniques such as vector embeddings and similarity search to identify the most pertinent data.\\n\\n2. **Generation Component**: After retrieving the relevant information, the generative model (often based on architectures like Transformers) synthesizes a coherent and contextually appropriate response. This model leverages the retrieved data to enhance its output, ensuring that the generated text is not only fluent but also factually grounded.\\n\\n#### Advantages\\n- **Enhanced Knowledge Access**: By incorporating external knowledge, RAG can provide more accurate and contextually relevant information than standalone generative models.\\n- **Dynamic Information Retrieval**: RAG systems can adapt to new information in real-time, making them suitable for applications where up-to-date knowledge is crucial.\\n- **Improved Contextual Understanding**: The retrieval process allows the model to better understand the context of the query, leading to more relevant and precise responses.\\n\\n#### Applications\\nRAG has a wide range of applications, including:\\n- **Question Answering**: Providing accurate answers to user queries by retrieving relevant documents and generating responses based on them.\\n- **Chatbots and Virtual Assistants**: Enhancing conversational agents with the ability to pull in real-time information from various sources.\\n- **Content Creation**: Assisting writers and content creators by providing relevant data and context for generating articles, reports, or creative writing.\\n\\n#### Challenges\\nDespite its advantages, RAG also faces several challenges:\\n- **Quality of Retrieved Information**: The effectiveness of the RAG model heavily depends on the quality and relevance of the retrieved documents. Poor retrieval can lead to inaccurate or misleading outputs.\\n- **Computational Complexity**: The dual process of retrieval and generation can be resource-intensive, requiring significant computational power and optimization.\\n- **Integration of Diverse Sources**: Ensuring that the model can effectively integrate and synthesize information from various types of sources can be complex.\\n\\n#### Conclusion\\nRetrieval-Augmented Generation represents a significant advancement in the field of natural language processing, merging the capabilities of information retrieval with generative models. As research and development in this area continue to evolve, RAG has the potential to transform various applications, making them more intelligent and responsive to user needs. Future work will likely focus on improving retrieval accuracy, optimizing computational efficiency, and expanding the range of applications for this promising technology.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = lcel_chain.invoke(\"retrieval augmented generation\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7E8_msHLpVH"
   },
   "source": [
    "The output format is slightly different, but the underlying functionality and content being output is the same. As before, we can view a formatted version of this output using the `Markdown` display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 997
    },
    "id": "42uPzB1JLpVH",
    "outputId": "0d7254e1-0cfe-43ff-dea2-dfad2f760007"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Report on Retrieval-Augmented Generation (RAG)\n",
       "\n",
       "#### Introduction\n",
       "Retrieval-Augmented Generation (RAG) is an innovative approach that combines the strengths of information retrieval and natural language generation. This method enhances the capabilities of language models by allowing them to access external knowledge sources, thereby improving the accuracy and relevance of generated responses.\n",
       "\n",
       "#### Concept Overview\n",
       "RAG operates on the principle of integrating a retrieval mechanism with a generative model. The process typically involves two main components:\n",
       "\n",
       "1. **Retrieval Component**: This part of the system retrieves relevant documents or pieces of information from a large corpus based on a given query. It uses techniques such as vector embeddings and similarity search to identify the most pertinent data.\n",
       "\n",
       "2. **Generation Component**: After retrieving the relevant information, the generative model (often based on architectures like Transformers) synthesizes a coherent and contextually appropriate response. This model leverages the retrieved data to enhance its output, ensuring that the generated text is not only fluent but also factually grounded.\n",
       "\n",
       "#### Advantages\n",
       "- **Enhanced Knowledge Access**: By incorporating external knowledge, RAG can provide more accurate and contextually relevant information than standalone generative models.\n",
       "- **Dynamic Information Retrieval**: RAG systems can adapt to new information in real-time, making them suitable for applications where up-to-date knowledge is crucial.\n",
       "- **Improved Contextual Understanding**: The retrieval process allows the model to better understand the context of the query, leading to more relevant and precise responses.\n",
       "\n",
       "#### Applications\n",
       "RAG has a wide range of applications, including:\n",
       "- **Question Answering**: Providing accurate answers to user queries by retrieving relevant documents and generating responses based on them.\n",
       "- **Chatbots and Virtual Assistants**: Enhancing conversational agents with the ability to pull in real-time information from various sources.\n",
       "- **Content Creation**: Assisting writers and content creators by providing relevant data and context for generating articles, reports, or creative writing.\n",
       "\n",
       "#### Challenges\n",
       "Despite its advantages, RAG also faces several challenges:\n",
       "- **Quality of Retrieved Information**: The effectiveness of the RAG model heavily depends on the quality and relevance of the retrieved documents. Poor retrieval can lead to inaccurate or misleading outputs.\n",
       "- **Computational Complexity**: The dual process of retrieval and generation can be resource-intensive, requiring significant computational power and optimization.\n",
       "- **Integration of Diverse Sources**: Ensuring that the model can effectively integrate and synthesize information from various types of sources can be complex.\n",
       "\n",
       "#### Conclusion\n",
       "Retrieval-Augmented Generation represents a significant advancement in the field of natural language processing, merging the capabilities of information retrieval with generative models. As research and development in this area continue to evolve, RAG has the potential to transform various applications, making them more intelligent and responsive to user needs. Future work will likely focus on improving retrieval accuracy, optimizing computational efficiency, and expanding the range of applications for this promising technology."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbutFcpyLpVI"
   },
   "source": [
    "### How Does the Pipe Operator Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIjOY7VILpVI"
   },
   "source": [
    "Before moving onto other LCEL features, let's take a moment to understand what the pipe operator `|` is doing and _how_ it works.\n",
    "\n",
    "Functionality wise, the pipe tells you that whatever the _left_ side outputs will be fed as input into the _right_ side. In the example of `prompt | llm | output_parser`, we see that `prompt` feeds into `llm` feeds into `output_parser`.\n",
    "\n",
    "The pipe operator is a way of chaining together components, and is a way of saying that whatever the _left_ side outputs will be fed as input into the _right_ side.\n",
    "\n",
    "Let's make a basic class named `Runnable` that will transform our a provided function into a _runnable_ class that we will then use with the pipe `|` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "9xTvwvF4LpVI"
   },
   "outputs": [],
   "source": [
    "class Runnable:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "    def __or__(self, other):\n",
    "        def chained_func(*args, **kwargs):\n",
    "            print(f\"Chaining {self.func.__name__} | {other.func.__name__}\")\n",
    "            print(f\"  with args={args}, kwargs={kwargs}\")\n",
    "            return other.invoke(self.func(*args, **kwargs))\n",
    "        return Runnable(chained_func)\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        print(f\"Invoking {self.func.__name__} with args={args}, kwargs={kwargs}\")\n",
    "        return self.func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2Vg_PhXLpVI"
   },
   "source": [
    "With the `Runnable` class, we will be able wrap a function into the class, allowing us to then chain together multiple of these _runnable_ functions using the `__or__` method.\n",
    "\n",
    "First, let's create a few functions that we'll chain together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "G7HCw9-VLpVI"
   },
   "outputs": [],
   "source": [
    "def add_five(x):\n",
    "    print(f\"add_five({x})\")\n",
    "    return x+5\n",
    "\n",
    "def sub_five(x):\n",
    "    print(f\"sub_five({x})\")\n",
    "    return x-5\n",
    "\n",
    "def mul_five(x):\n",
    "    print(f\"mul_five({x})\")\n",
    "    return x*5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0fDvmtWLpVI"
   },
   "source": [
    "Now we wrap our functions with the `Runnable`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "LfnBBcbxLpVI"
   },
   "outputs": [],
   "source": [
    "add_five_runnable = Runnable(add_five)\n",
    "sub_five_runnable = Runnable(sub_five)\n",
    "mul_five_runnable = Runnable(mul_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jHLbl_7LpVI"
   },
   "source": [
    "Finally, we can chain these together using the `__or__` method from the `Runnable` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BHFo61TwLpVI",
    "outputId": "3a03c07b-7ca9-45f5-ab2e-a979565b7e42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking chained_func with args=(3,), kwargs={}\n",
      "Chaining chained_func | mul_five\n",
      "  with args=(3,), kwargs={}\n",
      "Chaining add_five | sub_five\n",
      "  with args=(3,), kwargs={}\n",
      "add_five(3)\n",
      "Invoking sub_five with args=(8,), kwargs={}\n",
      "sub_five(8)\n",
      "Invoking mul_five with args=(3,), kwargs={}\n",
      "mul_five(3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (add_five_runnable).__or__(sub_five_runnable).__or__(mul_five_runnable)\n",
    "\n",
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oim1zWHLpVI"
   },
   "source": [
    "So we can see that we're able to chain together our functions using `__or__`. The pipe `|` operator is simply a shortcut for the `__or__` method, so we can create the exact same chain like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uh3-jF8pLpVJ",
    "outputId": "933ea922-f587-4f98-e825-b26113c5dc08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking chained_func with args=(3,), kwargs={}\n",
      "Chaining chained_func | mul_five\n",
      "  with args=(3,), kwargs={}\n",
      "Chaining add_five | sub_five\n",
      "  with args=(3,), kwargs={}\n",
      "Invoking sub_five with args=(8,), kwargs={}\n",
      "Invoking mul_five with args=(3,), kwargs={}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = add_five_runnable | sub_five_runnable | mul_five_runnable\n",
    "\n",
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Lnj00T8LpVJ"
   },
   "source": [
    "## LCEL `RunnableLambda`\n",
    "\n",
    "The `RunnableLambda` class is LangChain's built-in method for constructing a _runnable_ object from a function. That is, it does the same thing as the custom `Runnable` class we created earlier. Let's try it out with the same functions as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "BmMH8GzVLpVJ"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "add_five_runnable = RunnableLambda(add_five)\n",
    "sub_five_runnable = RunnableLambda(sub_five)\n",
    "mul_five_runnable = RunnableLambda(mul_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkpwIKq6LpVJ"
   },
   "source": [
    "We chain these together again with the pipe `|` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "9e58vaSELpVJ"
   },
   "outputs": [],
   "source": [
    "chain = add_five_runnable | sub_five_runnable | mul_five_runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FdqbdAyLpVJ"
   },
   "source": [
    "And call them using the `invoke` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqHBMT8ULpVJ",
    "outputId": "b32dcaf1-1207-4cd4-f881-d9ffd5a754d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_five(3)\n",
      "sub_five(8)\n",
      "mul_five(3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47mu4xxqLpVJ"
   },
   "source": [
    "Now we want to try something a little more testing, so this time we will generate a report, and we will try and edit that report using this functionallity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "xRJLzshqLpVJ"
   },
   "outputs": [],
   "source": [
    "prompt_str = \"give me a small report about {topic}\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "lfl_s4VALpVJ"
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WSMlRM8wLpVJ",
    "outputId": "2b25fd0b-8e2c-4513-9980-25f587eee58b"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Report on Artificial Intelligence (AI)\n",
       "\n",
       "#### Introduction\n",
       "Artificial Intelligence (AI) refers to the simulation of human intelligence in machines programmed to think and learn like humans. It encompasses a variety of technologies, including machine learning, natural language processing, robotics, and computer vision. AI has rapidly evolved over the past few decades, transforming industries and daily life.\n",
       "\n",
       "#### Current State of AI\n",
       "As of 2023, AI technologies are integrated into numerous applications, including:\n",
       "\n",
       "1. **Healthcare**: AI is used for diagnostics, personalized medicine, and predictive analytics. Machine learning algorithms analyze medical data to identify patterns and assist in treatment plans.\n",
       "\n",
       "2. **Finance**: AI algorithms are employed for fraud detection, algorithmic trading, and risk management. They analyze vast amounts of data to make real-time decisions.\n",
       "\n",
       "3. **Transportation**: Autonomous vehicles utilize AI for navigation and safety. AI systems process data from sensors to make driving decisions.\n",
       "\n",
       "4. **Customer Service**: Chatbots and virtual assistants powered by AI provide 24/7 customer support, handling inquiries and resolving issues efficiently.\n",
       "\n",
       "5. **Manufacturing**: AI optimizes supply chains, predicts maintenance needs, and enhances production processes through automation.\n",
       "\n",
       "#### Challenges and Ethical Considerations\n",
       "Despite its benefits, AI poses several challenges:\n",
       "\n",
       "- **Bias and Fairness**: AI systems can perpetuate existing biases present in training data, leading to unfair outcomes in areas like hiring and law enforcement.\n",
       "\n",
       "- **Privacy Concerns**: The use of AI in data collection raises significant privacy issues, as personal information can be misused or inadequately protected.\n",
       "\n",
       "- **Job Displacement**: Automation driven by AI may lead to job losses in certain sectors, necessitating workforce retraining and adaptation.\n",
       "\n",
       "- **Accountability**: Determining responsibility for decisions made by AI systems, especially in critical areas like healthcare and autonomous driving, remains a complex issue.\n",
       "\n",
       "#### Future Trends\n",
       "The future of AI is promising, with several trends expected to shape its development:\n",
       "\n",
       "- **Explainable AI**: There is a growing demand for AI systems that can provide transparent and understandable reasoning behind their decisions.\n",
       "\n",
       "- **AI in Creative Fields**: AI is increasingly being used in art, music, and content creation, leading to new forms of collaboration between humans and machines.\n",
       "\n",
       "- **Regulation and Governance**: As AI becomes more pervasive, governments and organizations are likely to implement regulations to ensure ethical use and mitigate risks.\n",
       "\n",
       "- **Human-AI Collaboration**: The focus will shift towards enhancing human capabilities through AI, fostering collaboration rather than replacement.\n",
       "\n",
       "#### Conclusion\n",
       "Artificial Intelligence is a transformative technology with the potential to revolutionize various sectors. While it offers significant benefits, addressing the associated challenges and ethical considerations is crucial for its responsible development and deployment. As AI continues to evolve, its impact on society will depend on how we navigate these complexities."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = chain.invoke(\"AI\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XE7HOxwaLpVJ"
   },
   "source": [
    "Here we are making two functions, `extract_fact` to pull out the main content of our text and `replace_word` that will replace AI with Skynet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Wc7ws8FkLpVJ"
   },
   "outputs": [],
   "source": [
    "def extract_fact(x):\n",
    "    if \"\\n\\n\" in x:\n",
    "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "old_word = \"AI\"\n",
    "new_word = \"skynet\"\n",
    "\n",
    "def replace_word(x):\n",
    "    return x.replace(old_word, new_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktHrg5iSLpVK"
   },
   "source": [
    "Lets wrap these functions and see what the output is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "_vFeqMY-LpVK"
   },
   "outputs": [],
   "source": [
    "extract_fact_runnable = RunnableLambda(extract_fact)\n",
    "replace_word_runnable = RunnableLambda(replace_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "TDWN-tQzLpVK"
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser | extract_fact_runnable | replace_word_runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 950
    },
    "id": "OCODP8vLLpVK",
    "outputId": "7d4c8a62-1b61-42fa-e6c3-f0dbe634f7f5"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Report on Retrieval-Augmented Generation (RAG)\n",
       "\n",
       "#### Introduction\n",
       "Retrieval-Augmented Generation (RAG) is an innovative approach that combines the strengths of information retrieval and natural language generation. This method enhances the capabilities of language models by allowing them to access external knowledge sources, thereby improving the accuracy and relevance of generated responses.\n",
       "\n",
       "#### Concept Overview\n",
       "RAG operates on the principle of integrating a retrieval mechanism with a generative model. The process typically involves two main components:\n",
       "\n",
       "1. **Retrieval Component**: This part of the system retrieves relevant documents or pieces of information from a large corpus based on a given query. It uses techniques such as vector embeddings and similarity search to identify the most pertinent data.\n",
       "\n",
       "2. **Generation Component**: After retrieving the relevant information, the generative model (often based on architectures like Transformers) synthesizes a coherent and contextually appropriate response. This model leverages the retrieved data to enhance its output, ensuring that the generated text is not only fluent but also factually grounded.\n",
       "\n",
       "#### Advantages\n",
       "- **Enhanced Accuracy**: By accessing up-to-date and domain-specific information, RAG can produce more accurate and contextually relevant responses compared to traditional generative models that rely solely on pre-existing training data.\n",
       "- **Dynamic Knowledge Integration**: RAG allows for the incorporation of new information without the need for retraining the entire model, making it adaptable to changing knowledge landscapes.\n",
       "- **Improved Contextual Understanding**: The retrieval process helps the model to better understand the context of the query, leading to more nuanced and informed responses.\n",
       "\n",
       "#### Applications\n",
       "RAG has a wide range of applications, including:\n",
       "- **Question Answering**: Providing precise answers to user queries by retrieving relevant documents and generating responses based on that information.\n",
       "- **Chatbots and Virtual Assistants**: Enhancing conversational agents with the ability to pull in real-time data, making interactions more informative and engaging.\n",
       "- **Content Creation**: Assisting writers and content creators by providing relevant information and suggestions based on current trends and data.\n",
       "\n",
       "#### Challenges\n",
       "Despite its advantages, RAG faces several challenges:\n",
       "- **Quality of Retrieved Information**: The effectiveness of the generation depends heavily on the quality and relevance of the retrieved documents. Poor retrieval can lead to inaccurate or misleading outputs.\n",
       "- **Computational Complexity**: The dual process of retrieval and generation can be resource-intensive, requiring significant computational power and optimization.\n",
       "- **Integration of Diverse Sources**: Ensuring that the retrieved information is coherent and seamlessly integrated into the generated text can be complex, especially when dealing with conflicting data.\n",
       "\n",
       "#### Conclusion\n",
       "Retrieval-Augmented Generation represents a significant advancement in the field of natural language processing, merging the capabilities of information retrieval with generative models. As research and development in this area continue to evolve, RAG is poised to enhance various applications, making AI systems more knowledgeable, responsive, and context-aware. Future work will likely focus on improving retrieval accuracy, reducing computational demands, and refining the integration of diverse information sources."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = chain.invoke(\"retrieval augmented generation\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHSumR1tLpVK"
   },
   "source": [
    "Those are our `RunnableLambda` functions. It's worth noting that all inputs to these functions are expected to be a SINGLE arguments. If you have a function that accepts multiple arguments, you can input a dictionary with keys, then unpack them inside the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIXKLyQKLpVK"
   },
   "source": [
    "## LCEL `RunnableParallel` and `RunnablePassthrough`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avFhxg_pLpVK"
   },
   "source": [
    "LCEL provides us with various `Runnable` classes that allow us to control the flow of data and execution order through our chains. Two of these are `RunnableParallel` and `RunnablePassthrough`.\n",
    "\n",
    "* `RunnableParallel` — allows us to run multiple `Runnable` instances in parallel. Acting almost as a Y-fork in the chain.\n",
    "\n",
    "* `RunnablePassthrough` — allows us to pass through a variable to the next `Runnable` without modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTsqIAufLpVK"
   },
   "source": [
    "To see these runnables in action, we will create two data sources, each source provides specific information but to answer the question we will need both to fed to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "zaD678FMLpVK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_10636\\3699332964.py:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding = OpenAIEmbeddings()\n",
      "d:\\Karol\\work\\Techniczne\\Szkolenia\\AurelioLabs_Langchain\\.venv\\Lib\\site-packages\\docarray\\helper.py:255: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  e.g. '\\*.py', '[\\*.zip, \\*.gz]'\n",
      "d:\\Karol\\work\\Techniczne\\Szkolenia\\AurelioLabs_Langchain\\.venv\\Lib\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "vecstore_a = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"half the info is here\",\n",
    "        \"DeepSeek-V3 was released in December 2024\"\n",
    "    ],\n",
    "    embedding=embedding\n",
    ")\n",
    "vecstore_b = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"the other half of the info is here\",\n",
    "        \"the DeepSeek-V3 LLM is a mixture of experts model with 671B parameters\"\n",
    "    ],\n",
    "    embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ub4pjRrBLpVK"
   },
   "source": [
    "Here you can see the prompt does have three inputs, two for context and one for the question itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "pnxXrw-CLpVK"
   },
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"Using the context provided, answer the user's question.\n",
    "Context:\n",
    "{context_a}\n",
    "{context_b}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "jL3YN1c1LpVK"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(prompt_str),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YchbLoKLpVK"
   },
   "source": [
    "Here we are wrapping our vector stores as retrievers so they can be fitted into one big retrieval variable to be used by the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "xkhFV8-SLpVL"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "retriever_a = vecstore_a.as_retriever()\n",
    "retriever_b = vecstore_b.as_retriever()\n",
    "\n",
    "retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"context_a\": retriever_a, \"context_b\": retriever_b, \"question\": RunnablePassthrough()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKtORU5xLpVL"
   },
   "source": [
    "The chain we'll be constructing will look something like this:\n",
    "\n",
    "![](https://github.com/aurelio-labs/langchain-course/blob/main/assets/lcel-flow.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "PNLpfS3_LpVL"
   },
   "outputs": [],
   "source": [
    "chain = retrieval | prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eo8BfprxLpVL"
   },
   "source": [
    "We `invoke` it as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "84gEXUG1LpVL",
    "outputId": "432d6af2-b59c-4dc5-d390-1ecbfc3efbd1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The DeepSeek-V3 model, released in December 2024, is a mixture of experts model with 671 billion parameters.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(\n",
    "    \"what architecture does the model DeepSeek released in december use?\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcwr16jsLpVL"
   },
   "source": [
    "With that we've seen how we can use `RunnableParallel` and `RunnablePassthrough` to control the flow of data and execution order through our chains.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "AurelioLabs_Langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
